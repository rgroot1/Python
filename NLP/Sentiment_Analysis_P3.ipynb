{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "## Part 3: Keras & LSTM\n",
    "\n",
    "In this notebook you will learn how to use Keras to build a neural network as well as the LSTM\n",
    "\n",
    "**Outline**:\n",
    "\n",
    "- Keras\n",
    "- RNN\n",
    "- LSTM\n",
    "\n",
    "*Some codes are adapted from [deeplearning.ai](https://www.deeplearning.ai/). Please do not use the code for ANY commercial use.*\n",
    "\n",
    "Make sure you've installed the following packages:\n",
    "\n",
    "- tensorflow\n",
    "- keras\n",
    "- nltk\n",
    "- pandas\n",
    "- h5py\n",
    "- emoji\n",
    "\n",
    "> If you're using `conda` as your package manager, you may noticed that `emoji` is not included in conda. To install it, you need to use `pip` instead:\n",
    "\n",
    "> 1. Activate your virtual environment: `source activate <your_venv>`.\n",
    "> 2. Verfiy that you're using `pip` along with the virtual environemnt: `which pip`.\n",
    "> 3. Install the package by `pip install emoji`. (Do not use `pip3`! There will be only one `pip` version inside your virtual environment.)\n",
    "> 4. Deactivate your virtual environment: `source deactivate` <your_venv>.\n",
    "\n",
    "**Pipeline**\n",
    "\n",
    "<img src=\"resources/pipeline.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import\n",
    "from ml_utils import *\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Activation\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_moon()\n",
    "\n",
    "# Plot the training set\n",
    "plt.scatter(\n",
    "    X_train[0,:],\n",
    "    X_train[1,:],\n",
    "    c=Y_train[0],\n",
    "    cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Keras:**\n",
    "\n",
    "1. Define the structure of the network. \n",
    "2. Print the summary of your network to see if shape and #of params is correct.\n",
    "3. **Compile the model**.\n",
    "4. Fit the model.\n",
    "5. Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=2, activation='tanh'))  # hidden layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # output layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/keras_network.png\" width=\"800\">\n",
    "\n",
    "<center>*Keras 2-layer neural network*</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train.T, \n",
    "    Y_train.T, \n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train.T, Y_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test.T, Y_test.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Notes:**</span>\n",
    "\n",
    "Another good practice to create a model is to treat each layer as a \"transformer\" or a \"function\" that helps us to map the input (training features) to the output (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_nn_model():\n",
    "    X = Input(shape=(2,))\n",
    "    Z = Dense(4, activation='tanh')(X)\n",
    "    Y = Dense(1, activation='sigmoid')(Z)\n",
    "    return Model(inputs=X, outputs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = simple_nn_model()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more intuitive explanation of LSTM, you may refer to [this post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import \n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/deep_lstm.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> A 2-layer LSTM sequence classifier. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_emoji()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load pretrained word embeddings**\n",
    "\n",
    "2 dictionaries are loaded:\n",
    "\n",
    "- `word_to_index`: map a word to its index in the vocabulary\n",
    "    - Example:  `'word' -> 1234`\n",
    "\n",
    "- `word_to_vec_map`: map a word to its embedding\n",
    "    - Example: `'word' -> [0.1, 0.2, ..., 0.45]`\n",
    "\n",
    "When adding a custom embedding layer in Keras, we can only load the pretrained embedding as a big matrix instead of a dictionary. An index will help us locate the entry for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentence to the index array\n",
    "X_tmp = np.array([\"I like it\"])\n",
    "sentences_to_indices(X_tmp, word_to_index, max_len = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Notes:**</span>: `sentences_to_indices` is provided in `ml_utils`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer\n",
    "\n",
    "We need to build a embedding matrix where each row represent a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Build and return a Keras Embedding Layer given word_to_vec mapping and word_to_index mapping\n",
    "    \n",
    "    Args:\n",
    "        word_to_vec_map (dict[str->np.ndarray]): map from a word to a vector with shape (N,) where N is the length of a word vector (50 in our case)\n",
    "        word_to_index (dict[str->int]): map from a word to its index in vocabulary\n",
    "\n",
    "    Return:\n",
    "        Keras.layers.Embedding: Embedding layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keras requires vocab length start from index 1\n",
    "    vocab_len = len(word_to_index) + 1  \n",
    "    emb_dim = list(word_to_vec_map.values())[0].shape[0]\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    return Embedding(\n",
    "        input_dim=vocab_len, \n",
    "        output_dim=emb_dim, \n",
    "        trainable=False,  # Indicating this is a pre-trained embedding \n",
    "        weights=[emb_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For more information on how to define a pre-trained embedding layer in Keras, please refer to [this post](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Build and return the Keras model\n",
    "    \n",
    "    Args:\n",
    "        input_shape (np.ndarray): The shape of input layer, usually it means (#training_example, max_len)\n",
    "        word_to_vec_map (dict[str->np.ndarray]): map from a word to a vector with shape (N,) where N is the length of a word vector (50 in our case)\n",
    "        word_to_index (dict[str->int]): map from a word to its index in vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        Keras.models.Model: 2-layer LSTM model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # 2-layer LSTM\n",
    "    X = LSTM(128, return_sequences=True, recurrent_dropout=0.5)(embeddings)  # N->N RNN\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, recurrent_dropout=0.5)(X)  # N -> 1 RNN\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5, activation='softmax')(X)\n",
    "    \n",
    "    # Create and return model\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = len(max(X_train, key=len).split())\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = emoji_model((maxlen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training/testing features into index list\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxlen)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxlen)\n",
    "\n",
    "# Convert training/testing labels into one hot array\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_indices, \n",
    "    Y_train_oh, \n",
    "    epochs = 50, \n",
    "    batch_size = 32, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_train_indices, Y_train_oh)\n",
    "print('loss = %.4f, acc = %.2f%%' % (loss, acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print('loss = %.4f, acc = %.2f%%' % (loss, acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save & Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two parts need to be saved inorder to use the model in prod:\n",
    "\n",
    "1. Neural Network Structure\n",
    "2. Trained Weights (Matrix)\n",
    "\n",
    "We will save them separately. This makes it easy to manage multiple versions of weights and you can always choose which version to go for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use JSON to store model structure and h5py to store compressed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model structure as json\n",
    "with open(\"emoji_model.json\", \"w\") as fp:\n",
    "    fp.write(model.to_json())\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(\"emoji_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse will load the model structure and trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# Load model structure\n",
    "with open(\"emoji_model_best.json\", \"r\") as fp:\n",
    "    model = model_from_json(fp.read())\n",
    "\n",
    "# Load model weights\n",
    "model.load_weights(\"emoji_model_best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_train_indices, Y_train_oh)\n",
    "print('loss = %.4f, acc = %.2f%%' % (loss, acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print('loss = %.4f, acc = %.2f%%' % (loss, acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([\"i am not feeling happy\"])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxlen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TODO: IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=80)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=80)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In production, you should split your data into 3 parts: training data, validation data and test data. You should not feed `validation_data` with test data like we did here. This is only for quick test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save model structure\n",
    "with open(\"imdb_model.json\", \"w\") as fp:\n",
    "    fp.write(model.to_json())\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(\"imdb_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Use our pre-trained model to replace the Embedding layer and train the model for 30 epochs.\n",
    "- Collect the training history data\n",
    "- Plot the accuracy and loss\n",
    "- Find the best epoch number to stop traning\n",
    "- Retrain the model and save it for later use.\n",
    "\n",
    "\n",
    "The model summary should be similar to:\n",
    "\n",
    "```plain\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_34 (Embedding)     (None, None, 50)          20000050  \n",
    "_________________________________________________________________\n",
    "lstm_56 (LSTM)               (None, 128)               91648     \n",
    "_________________________________________________________________\n",
    "dense_39 (Dense)             (None, 1)                 129       \n",
    "=================================================================\n",
    "Total params: 20,091,827\n",
    "Trainable params: 91,777\n",
    "Non-trainable params: 20,000,050\n",
    "_________________________________________________________________\n",
    "```\n",
    "\n",
    "The plot should be similar to:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"resources/acc.png\"></td>\n",
    "        <td><img src=\"resources/loss.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(pretrained_embedding_layer(word_to_vec_map, word_to_index))\n",
    "model2.add(LSTM(128, dropout=0.3, recurrent_dropout=0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "history = model2.fit(x_train, \n",
    "                     y_train,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=30,\n",
    "                     validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h = history.history.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(h['acc'])\n",
    "plt.plot(h['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(h['loss'])\n",
    "plt.plot(h['val_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
