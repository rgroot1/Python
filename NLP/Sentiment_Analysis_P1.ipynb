{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f03cff31-83e1-4954-9b8e-7b22ffe5e08a"
    }
   },
   "source": [
    "# DS1801 Project: Sentiment Analysis\n",
    "\n",
    "## Part 1: Naive Bayes\n",
    "\n",
    "In this notebook, you will learn how to use different packages in Python to build a complete pipeline for solving sentiment analysis problem. We will be using Mutinomial NB in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dadf36f5-94ca-4ad6-82e5-eee766bbe079"
    }
   },
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e28f0511-3ad7-4f7b-a9c2-7667d0f738d3"
    }
   },
   "source": [
    "<img src=\"pipeline.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4993d77d-4107-4ee2-b456-537ad84b6927"
    }
   },
   "source": [
    "### Get Familiar with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bafb6e90-d675-4161-ab5d-23c386f8762d"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "eaa70d4f-ef54-46ad-b690-6aa39d648271"
    }
   },
   "outputs": [],
   "source": [
    "# Load csv file into DataFrame\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f01fc173-125a-467a-b40d-89ea16cf2bd9"
    }
   },
   "outputs": [],
   "source": [
    "train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6d5c2796-9d11-4cd0-8d55-fd2f9bcc02e8"
    }
   },
   "outputs": [],
   "source": [
    "print(\"sentiment  :\", train.sentiment[0])\n",
    "print(\"reviewText :\", train.reviewText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "490495cc-6ecb-4415-b13c-4cd6ea380b6e"
    }
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "3be81287-1de0-4103-99b5-854c35c53159"
    }
   },
   "outputs": [],
   "source": [
    "# Get a sample (head) of the data frame\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8bda0af1-6a24-4cd9-97ec-d4bff3746662"
    }
   },
   "outputs": [],
   "source": [
    "# Statics on tags\n",
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "21638fdf-767a-4ccf-9328-fc01542c924e"
    }
   },
   "source": [
    "See [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/10min.html?highlight=data%20frame) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a836d02c-6189-4fff-9abf-6f058e01ccae"
    }
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6ca3bbf5-1429-4f1b-8293-cf87bb415b8c"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    x = data['reviewText'].tolist()\n",
    "    y = data['sentiment'].tolist()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "52e9917d-85f9-4bcf-b97f-d686220f5d1f"
    }
   },
   "outputs": [],
   "source": [
    "train_x, train_y = load_data('train.csv')\n",
    "test_x, test_y = load_data('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a13689ea-e62d-4d42-8527-783e42fc2302"
    }
   },
   "outputs": [],
   "source": [
    "print('training size:', len(train_x))\n",
    "print('test size:', len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e2e6ba14-7fea-4c02-88e9-f87688e70238"
    }
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0300719b-d5df-4b2a-8b30-97eb486dfd08"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "transtbl = str.maketrans(string.punctuation, ' ' * len(string.punctuation)) # Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "dd71b757-9090-4aec-874f-3a972cef6b7c"
    }
   },
   "outputs": [],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8f60988b-d40b-421a-9e35-16471359736e"
    }
   },
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c16c7bfa-f53a-4d52-bfab-cc1d4edad5ea"
    }
   },
   "outputs": [],
   "source": [
    "'ababc'.translate(str.maketrans('abc','def'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c2cda7c1-8f58-4e53-a343-b6ba010238f7"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(line):\n",
    "    line = line.replace('<br />', '')   # Remove html tag (<br />)\n",
    "    line = line.translate(transtbl)     # Remove punctuation\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = []\n",
    "    for t in nltk.word_tokenize(line):\n",
    "        t = t.lower()\n",
    "        if t not in stopwords:\n",
    "            lemma = lemmatizer.lemmatize(t, 'v')\n",
    "            tokens.append(lemma)\n",
    "            \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6cf378f3-f9a2-466c-8993-e8b08ae1959a"
    }
   },
   "outputs": [],
   "source": [
    "# Yet a more compact way to write the code\n",
    "def preprocessing(line: str) -> str:\n",
    "    line = line.replace('<br />', '').translate(transtbl)\n",
    "    \n",
    "    tokens = [lemmatizer.lemmatize(t.lower(),'v')  # What to put in the list\n",
    "              for t in nltk.word_tokenize(line)    # Where \n",
    "              if t.lower() not in stopwords]       # If\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a7867dff-ea5b-41c8-94fb-702b34ebdce1"
    }
   },
   "outputs": [],
   "source": [
    "test_str = \"I bought several books yesterday<br /> and I really love them!\"\n",
    "preprocessing(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8a025157-04cd-4a86-adbf-f9fba19a007b"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "3fe5ba36-1a7b-4692-be28-62eb2ce57e59"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess all data\n",
    "train_x = [preprocessing(x) for x in train_x]\n",
    "test_x = [preprocessing(x) for x in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "95739710-f7fc-4c8a-a587-e4e58e3cdc81"
    }
   },
   "outputs": [],
   "source": [
    "# Yet a more modern way to write code\n",
    "train_x = list(map(preprocessing, train_x))\n",
    "test_x = list(map(preprocessing, test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0b72730c-2d0d-4238-8d5b-f4601c921906"
    }
   },
   "outputs": [],
   "source": [
    "# multi-task function\n",
    "from nbmultitask import ThreadWithLogAndControls\n",
    "from time import sleep\n",
    "\n",
    "def preprocess_dataset(thread_print, datas, output):\n",
    "    \"\"\"\n",
    "    Preprocess dataset in the background thread, so that\n",
    "    it won't block the notebook from running other code snippets.\n",
    "    Do not update output anywhere outside this background thread.\n",
    "\n",
    "    Args:\n",
    "        thread_print: for printing in nbmultitask, necessary\n",
    "        datas: dict containing all datasets to be processed\n",
    "        output: shared variable for storing output\n",
    "    \"\"\"\n",
    "    \n",
    "    sleep(0.5) # For consistent output format\n",
    "    \n",
    "    for name, data in datas.items():\n",
    "        output[name] = []\n",
    "        thread_print(\"\\nPreprocessing \" + name)\n",
    "        \n",
    "        m = len(data) / 4 # print progress every 25%\n",
    "        for i, x in enumerate(data):\n",
    "            output[name].append(preprocessing(x))\n",
    "            if (i + 1) % m == 0:\n",
    "                thread_print(\"Processed: %d%%\" % ((i + 1) / m * 25))\n",
    "        thread_print(\"Done.\")\n",
    "    \n",
    "    thread_print(\"All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "78e3d75e-e38b-4b1c-8584-18110fd537c1"
    }
   },
   "outputs": [],
   "source": [
    "# dict to store output\n",
    "out = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d8d6ecba-334a-43e8-892e-4304e7327d7d"
    }
   },
   "outputs": [],
   "source": [
    "def multitask_wrapper(thread_print):\n",
    "    preprocess_dataset(thread_print, {\"Training data\": train_x, \"Testing data\": test_x}, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "51ce0872-464a-46dd-8d13-9ebdf6e5532f"
    }
   },
   "outputs": [],
   "source": [
    "task = ThreadWithLogAndControls(target=multitask_wrapper, name=\"Preprocessing Data\")\n",
    "task.control_panel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6fa1109d-54a9-4cc1-9fe5-42f33a81acc4"
    }
   },
   "outputs": [],
   "source": [
    "train_x = out['Training data']\n",
    "test_x = out['Testing data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c2521971-5068-4d71-9159-72c250950275"
    }
   },
   "source": [
    "### Some modern functions to introduce\n",
    "- map\n",
    "- reduce\n",
    "- filter\n",
    "\n",
    "They are very useful when running the project on a cluster or distributed compute system like Hadoop or Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "d5c32cd7-387a-425f-8c41-652d785bdf84"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4284e312f13a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Some useful modern functions\n",
    "l = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "# Map\n",
    "def square(x: int) -> int:\n",
    "    return x * x\n",
    "\n",
    "print(list(map(square,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( list(map(square, l)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6a12f8cc-9481-4f4c-9822-b050ea29f247"
    }
   },
   "outputs": [],
   "source": [
    "# Using lambda function\n",
    "print( list(map(lambda x: x * x, l)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c298eb40-d032-4a8c-a435-af30e28c267d"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce\n",
    "# reduce function is moved to functools\n",
    "def add(x: int, y: int) -> int:\n",
    "    return x + y\n",
    "\n",
    "import functools\n",
    "rst = functools.reduce(add, l)\n",
    "\n",
    "print (\"reduce\", l, \"by add:\", rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4e9175f1-3ca0-4251-83eb-73a42ced5a6f"
    }
   },
   "outputs": [],
   "source": [
    "# Using lambda function\n",
    "# reduce is moved to functools in Python 3\n",
    "rst = functools.reduce(lambda x, y: x + y, l)\n",
    "print (\"reduce\", l, \"by add:\", rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e9916a1d-6cfe-4ecb-b953-a74f959b5272"
    }
   },
   "outputs": [],
   "source": [
    "rst = functools.reduce(lambda x, y: min(x, y), l)\n",
    "print (\"reduce\", l, \"by min:\", rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1dfca4b5-5c00-40b9-b537-7761540fd1a0"
    }
   },
   "outputs": [],
   "source": [
    "# Filter\n",
    "# Much faster than loop, similar with list comprehension\n",
    "list(filter(lambda x: x < 5, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4eeefbfd-9a30-4e11-9647-daa037e26042"
    }
   },
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6e0d03c1-ad52-42a1-91d9-bc705a89c245"
    }
   },
   "outputs": [],
   "source": [
    "# Push all tokens and compute frequency of words\n",
    "all_words = []\n",
    "for line in train_x:\n",
    "    words = line.split()\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "        \n",
    "voca = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f3befe75-935e-479c-b78e-a739199a7a95"
    }
   },
   "outputs": [],
   "source": [
    "# Yet another more python-y style\n",
    "all_words = [w for line in train_x for w in line.split()]\n",
    "voca = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e9e13deb-7fad-47d9-a5d3-9203e14e721c"
    }
   },
   "outputs": [],
   "source": [
    "print(voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e87733fb-34db-482b-a21a-acb88c5b3e93"
    }
   },
   "outputs": [],
   "source": [
    "voca.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9fb9823c-b17b-4a6b-a478-ee9a923148e8"
    }
   },
   "outputs": [],
   "source": [
    "topwords = [fpair[0] for fpair in list(voca.most_common(10000))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9d86b026-19b4-47e1-a693-88f434dfb59e"
    }
   },
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "02fe7ca4-bcdf-4dc3-a066-4def28483597"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "eb6794ac-150b-41a7-8b7d-2e9934db9d74"
    }
   },
   "outputs": [],
   "source": [
    "cnt_vec = CountVectorizer()\n",
    "cnt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "3612dc45-26ad-4989-ab82-e6a44db89298"
    }
   },
   "outputs": [],
   "source": [
    "# Create our BAG of words (specify words we care about)\n",
    "cnt_vec.fit(topwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6c003f7f-2a78-47a2-ada2-a212f75b6517"
    }
   },
   "source": [
    "#### Tfâ€“idf term weighting: \n",
    "use a TF-IDF score (Term Frequency, Inverse Document Frequency) on top of our Bag of Words model. TF-IDF weighs words by how rare they are in our dataset, discounting words that are too frequent and just add to the noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6e37f508-9785-4250-a1e1-1354dfcff4c2"
    }
   },
   "source": [
    "- Tf: term-frequency\n",
    "- idf: inverse document-frequency\n",
    "- Tf-idf = $tf(t,d) \\times idf(t)$\n",
    "\n",
    "$$\n",
    "idf(t) = log{\\frac{1 + n_d}{1 + df(d, t)}} + 1\n",
    "$$\n",
    "\n",
    "![](http://www.onemathematicalcat.org/Math/Algebra_II_obj/Graphics/log_base_gt1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5f349274-78e7-41be-968b-ca1a4fb80d50"
    }
   },
   "source": [
    "> Sentence 1: The boy **love** the toy\n",
    "\n",
    "> Sentence 2: The boy **hate** the toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "12d18c1c-3899-4ea6-a677-2506a5f1d8a0"
    }
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "21537231-5ff9-4a7c-a54f-8ed911c88329"
    }
   },
   "outputs": [],
   "source": [
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "90c1cbe3-c5b2-4e80-8c88-4f0ec45b2e8a"
    }
   },
   "outputs": [],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a8bae77b-721c-4fa6-a10b-051574086736"
    }
   },
   "source": [
    "<span style=\"color:red\">**Tips:**</span>\n",
    "\n",
    "tf-idfs are computed slightly different in sklearn, where:\n",
    "\n",
    "$$\n",
    "idf(t) = log{\\frac{n_d}{1 + df(d, t)}}\n",
    "$$\n",
    "\n",
    "With `smooth_idf=True` set to `True`, the formula is:\n",
    "\n",
    "$$\n",
    "idf(t) = log{\\frac{n_d}{df(d, t)}} + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e07ae14e-e1e8-4bcd-8986-da2d14b2d3ba"
    }
   },
   "outputs": [],
   "source": [
    "# Since CountVectorizer and TfidTransformer are often used together\n",
    "# There is a class named TfidfVectorizer that combine these two steps\n",
    "tf_vec = TfidfVectorizer()\n",
    "tf_vec.fit(topwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a396d14b-1f38-44db-b25f-d457fe3998a7"
    }
   },
   "outputs": [],
   "source": [
    "t_corpus = ['the boy love the toy', 'the boy hate the toy'] # Voc = ['boy', 'hate', 'love', 'the', 'toy']\n",
    "t_cnt_vec = CountVectorizer()\n",
    "t_cnt_vec.fit(' '.join(t_corpus).split())\n",
    "t_cnt_vec.transform(t_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a2e931ef-7434-4014-9f09-57a2b684f6d5"
    }
   },
   "outputs": [],
   "source": [
    "# Tfidf on the test corpus\n",
    "t_tfidf_vec = TfidfVectorizer()\n",
    "t_tfidf_vec.fit(' '.join(t_corpus).split())\n",
    "t_tfidf_vec.transform(t_corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ee88a760-c8c0-4c21-92b0-bc6235086445"
    }
   },
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "5c70b73c-b077-4c0d-9e8f-935dcccb3782"
    }
   },
   "outputs": [],
   "source": [
    "# Extract features from training set\n",
    "# Vocabulary is from topwords\n",
    "train_features = tf_vec.transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "79855adf-c8c9-4b30-bca2-d729798de62e"
    }
   },
   "outputs": [],
   "source": [
    "# Array[n_train_data * n_features]\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7c6f787e-5013-466a-869d-bf4b76f681e9"
    }
   },
   "outputs": [],
   "source": [
    "tf_vec = TfidfVectorizer(vocabulary=topwords)\n",
    "train_features = tf_vec.fit_transform(train_x)\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "64ba5bc9-96ea-4829-8a7e-cf5d01781087"
    }
   },
   "outputs": [],
   "source": [
    "# Extract features from test set\n",
    "test_features = tf_vec.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "71abd6d5-05a7-4075-bd7a-f49a07d2f1d7"
    }
   },
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4ecfbfb7-9b24-499f-a4b4-087e3183b7ee"
    }
   },
   "source": [
    "### [Multinomial NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for **classification with discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "eef54d8f-0863-4285-bf5a-fa301dff8678"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "08172515-7b9a-4a38-967b-fcea2ac03b4b"
    }
   },
   "outputs": [],
   "source": [
    "mnb_model = MultinomialNB()\n",
    "mnb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2e1f0988-c008-441d-a60a-797620243dce"
    }
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "mnb_model.fit(train_features, train_y)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Multinomial NB model trained in %f seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f1a1e3f4-225f-4a59-ab02-82c874289bb1"
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred = mnb_model.predict(test_features)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "42e5eacf-8353-47df-93db-72ebe1a713a8"
    }
   },
   "outputs": [],
   "source": [
    "# Metrics\n",
    "# metrics.accuracy_score(y_true, y_pred)\n",
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(pred,test_y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1dc8a4ad-96e4-41f3-aeda-446c0799c0da"
    }
   },
   "outputs": [],
   "source": [
    "# Use keyword arguments to set arguments explicitly\n",
    "print(metrics.classification_report(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fcf0efe6-291b-4d03-a7d1-2ec665279f1e"
    }
   },
   "outputs": [],
   "source": [
    "# Example from sklearn documentation\n",
    "\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(metrics.classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1bcd8bdb-bdb9-401a-b9ce-e29ccc320f83"
    }
   },
   "source": [
    "### Predict new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1c0addbf-c65c-4d4a-89aa-fab535a72d35"
    }
   },
   "outputs": [],
   "source": [
    "# Predict a new sentence\n",
    "# vectorizer needs to be pre-fitted\n",
    "# At the end of the project, the function signature should be something like:\n",
    "# predict_new(sentent: str, vec, model) -> str\n",
    "\n",
    "def predict_new(sentence: str):\n",
    "    sentence = preprocessing(sentence)\n",
    "    features = tf_vec.transform([sentence])\n",
    "    pred = mnb_model.predict(features)\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "609df79f-52c5-42af-b849-e745afa397f7"
    }
   },
   "outputs": [],
   "source": [
    "predict_new('It looks nice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5ae16efd-71c1-4465-a247-33af0bb9246a"
    }
   },
   "source": [
    "### Select Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2d652ee5-fbc8-489b-8003-a79bd96d23db"
    }
   },
   "outputs": [],
   "source": [
    "def train_with_n_topwords(n: int, tfidf=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Train and get the accuracy with different model settings\n",
    "    Args:\n",
    "        n: number of features (top frequent words in the vocabulary)\n",
    "        tfidf: whether do tf-idf re-weighting or not\n",
    "    Outputs:\n",
    "        tuple: (accuracy score, classifier, vectorizer)\n",
    "    \"\"\"\n",
    "    topwords = [fpair[0] for fpair in list(voca.most_common(n))]\n",
    "    \n",
    "    if tfidf:\n",
    "        vec = TfidfVectorizer(vocabulary=topwords)\n",
    "    else:\n",
    "        vec = CountVectorizer(vocabulary=topwords)\n",
    "    \n",
    "    # Generate feature vectors\n",
    "    train_features = vec.fit_transform(train_x)\n",
    "    test_features  = vec.transform(test_x)\n",
    "    \n",
    "    # NB\n",
    "    mnb_model = MultinomialNB()\n",
    "    mnb_model.fit(train_features, train_y)\n",
    "    \n",
    "    # Test predict\n",
    "    pred = mnb_model.predict(test_features)\n",
    "    \n",
    "    return metrics.accuracy_score(pred, test_y), mnb_model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2505b8b5-0bba-4cca-8ba0-93cac0501e3e"
    }
   },
   "outputs": [],
   "source": [
    "train_with_n_topwords(500, tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1790e0a2-cf5d-45e4-a19a-e24005be2db1"
    }
   },
   "outputs": [],
   "source": [
    "from my_utils import print_progress\n",
    "\n",
    "possible_n = [500 * i for i in range(1, 20)]\n",
    "\n",
    "cnt_accuracies = []\n",
    "tfidf_accuracies = []\n",
    "\n",
    "for i, n in enumerate(possible_n):\n",
    "    cnt_accuracies.append(train_with_n_topwords(n)[0])\n",
    "    print_progress(bar_length=50, decimals=0, iteration=2 * i + 1, total=2*len(possible_n), prefix='Train and verify:')\n",
    "    \n",
    "    tfidf_accuracies.append(train_with_n_topwords(n, tfidf=True)[0])\n",
    "    print_progress(bar_length=50, decimals=0, iteration=2 * i + 2, total=2*len(possible_n), prefix='Train and verify:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4cb349db-39ae-4311-8521-1e055907abdd"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(possible_n, cnt_accuracies, label='Word Count')\n",
    "plt.plot(possible_n, tfidf_accuracies, label='Tf-idf')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fb560459-df66-41b6-8a64-842622e4aa79"
    }
   },
   "source": [
    "**Expected**:\n",
    "\n",
    "<img src=\"plot.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "db997a18-0447-456d-a77f-d0f7a8c0b39a"
    }
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = train_with_n_topwords(3000, tfidf=True) # best = (acc, model, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "5558a033-1f08-45b4-8abe-ed974d685831"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save vectorizer\n",
    "with open('tf_vec.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(best[2], pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "df3e7a2f-f4b7-4b04-bab3-552c97294acd"
    }
   },
   "outputs": [],
   "source": [
    "with open('mnb_model.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(best[1], pkl_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
